{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNCStTwPSc5jziakmh1EtEQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tilly963963/pytorch/blob/main/chapter2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy-CyQ6gQM3v"
      },
      "source": [
        "batch_n = 100\n",
        "in_features = 3\n",
        "out_features = 1\n",
        "epoch_n = 100\n",
        "learning_rate = 0.001 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjwbOoQ7CQXK",
        "outputId": "e467a0de-dc59-4dcf-b9b8-8c64403953b4"
      },
      "source": [
        "import torch\n",
        "#y = 3x_1+5x_2+1x3 自己產生資料\n",
        "x = torch.rand(batch_n, in_features)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2009, 0.3444, 0.3996],\n",
              "        [0.7072, 0.4946, 0.9411],\n",
              "        [0.6264, 0.3823, 0.3768],\n",
              "        [0.7723, 0.9557, 0.4762],\n",
              "        [0.6574, 0.9859, 0.9443],\n",
              "        [0.4154, 0.7574, 0.5344],\n",
              "        [0.6634, 0.0578, 0.6755],\n",
              "        [0.9193, 0.0234, 0.4609],\n",
              "        [0.5849, 0.8969, 0.0676],\n",
              "        [0.3636, 0.4409, 0.1208],\n",
              "        [0.2238, 0.0221, 0.6231],\n",
              "        [0.3493, 0.0730, 0.0441],\n",
              "        [0.0416, 0.8763, 0.3870],\n",
              "        [0.1994, 0.9211, 0.7748],\n",
              "        [0.8093, 0.4471, 0.8744],\n",
              "        [0.5156, 0.0870, 0.0078],\n",
              "        [0.0381, 0.4723, 0.5760],\n",
              "        [0.1930, 0.6640, 0.5725],\n",
              "        [0.3713, 0.4114, 0.2404],\n",
              "        [0.8498, 0.7397, 0.3123],\n",
              "        [0.2182, 0.5900, 0.9431],\n",
              "        [0.4754, 0.2229, 0.2020],\n",
              "        [0.7678, 0.1806, 0.4231],\n",
              "        [0.5289, 0.5574, 0.4428],\n",
              "        [0.0120, 0.3597, 0.9665],\n",
              "        [0.7761, 0.9484, 0.2305],\n",
              "        [0.4667, 0.3362, 0.3916],\n",
              "        [0.6277, 0.8523, 0.8722],\n",
              "        [0.9182, 0.5209, 0.1526],\n",
              "        [0.7478, 0.9826, 0.7572],\n",
              "        [0.7133, 0.5157, 0.6095],\n",
              "        [0.3489, 0.5079, 0.5043],\n",
              "        [0.4055, 0.7243, 0.3586],\n",
              "        [0.7610, 0.7052, 0.6530],\n",
              "        [0.5564, 0.2846, 0.5537],\n",
              "        [0.9857, 0.0277, 0.3069],\n",
              "        [0.9331, 0.2805, 0.2420],\n",
              "        [0.3522, 0.1912, 0.7577],\n",
              "        [0.9785, 0.0455, 0.3675],\n",
              "        [0.7786, 0.6539, 0.1543],\n",
              "        [0.5491, 0.0149, 0.6439],\n",
              "        [0.0857, 0.1980, 0.5207],\n",
              "        [0.4173, 0.0567, 0.5891],\n",
              "        [0.7364, 0.4530, 0.5687],\n",
              "        [0.7667, 0.4828, 0.8908],\n",
              "        [0.2679, 0.4412, 0.7296],\n",
              "        [0.8365, 0.3740, 0.2226],\n",
              "        [0.5411, 0.2931, 0.3356],\n",
              "        [0.6054, 0.9201, 0.4246],\n",
              "        [0.6126, 0.2882, 0.9518],\n",
              "        [0.9994, 0.4174, 0.7357],\n",
              "        [0.1280, 0.6987, 0.9766],\n",
              "        [0.1339, 0.7955, 0.1235],\n",
              "        [0.0691, 0.9888, 0.0311],\n",
              "        [0.7589, 0.7593, 0.3972],\n",
              "        [0.6889, 0.7402, 0.6178],\n",
              "        [0.6735, 0.3088, 0.1142],\n",
              "        [0.2673, 0.8287, 0.9320],\n",
              "        [0.3751, 0.9006, 0.4353],\n",
              "        [0.3384, 0.6178, 0.0343],\n",
              "        [0.9106, 0.8708, 0.3172],\n",
              "        [0.2859, 0.3830, 0.0482],\n",
              "        [0.0753, 0.2794, 0.4820],\n",
              "        [0.1027, 0.4969, 0.3725],\n",
              "        [0.8429, 0.1275, 0.7466],\n",
              "        [0.9863, 0.9133, 0.2102],\n",
              "        [0.4772, 0.4453, 0.0165],\n",
              "        [0.0150, 0.8654, 0.0737],\n",
              "        [0.0049, 0.5649, 0.7238],\n",
              "        [0.0364, 0.6454, 0.3106],\n",
              "        [0.5798, 0.5347, 0.6320],\n",
              "        [0.5599, 0.0198, 0.9139],\n",
              "        [0.5148, 0.5405, 0.1610],\n",
              "        [0.1690, 0.7145, 0.8539],\n",
              "        [0.0482, 0.0567, 0.7305],\n",
              "        [0.0297, 0.6605, 0.2372],\n",
              "        [0.8486, 0.2445, 0.5780],\n",
              "        [0.2700, 0.7165, 0.3451],\n",
              "        [0.9379, 0.5788, 0.2758],\n",
              "        [0.1313, 0.9627, 0.1608],\n",
              "        [0.6972, 0.5962, 0.2416],\n",
              "        [0.6364, 0.4906, 0.4438],\n",
              "        [0.0043, 0.3743, 0.5810],\n",
              "        [0.4788, 0.7022, 0.1961],\n",
              "        [0.0076, 0.5547, 0.1472],\n",
              "        [0.7415, 0.4722, 0.5089],\n",
              "        [0.0877, 0.6707, 0.1440],\n",
              "        [0.7662, 0.6150, 0.3778],\n",
              "        [0.5393, 0.5149, 0.5312],\n",
              "        [0.0330, 0.9013, 0.3055],\n",
              "        [0.6773, 0.9423, 0.0499],\n",
              "        [0.1249, 0.1040, 0.9577],\n",
              "        [0.9313, 0.5802, 0.8596],\n",
              "        [0.1792, 0.5306, 0.2228],\n",
              "        [0.5522, 0.1932, 0.0192],\n",
              "        [0.8397, 0.8467, 0.2021],\n",
              "        [0.3983, 0.6061, 0.3280],\n",
              "        [0.0366, 0.1563, 0.2233],\n",
              "        [0.7514, 0.7475, 0.1094],\n",
              "        [0.5772, 0.7561, 0.3401]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmr_QaVeCFWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d093b0-9511-4256-f8d9-8bac82eeb004"
      },
      "source": [
        "# #y = 3x_1+5x_2+1x3\n",
        "# x = torch.rand(batch_n, in_features)\n",
        "\n",
        "c = torch.Tensor([[3.],[5.],[1.]])#w\n",
        "y = x.mm(c)#100,3 x 3,1 => 100,1\n",
        "y = y.add(torch.rand(batch_n, out_features))#bais\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.0162],\n",
              "        [5.7881],\n",
              "        [4.4039],\n",
              "        [8.3968],\n",
              "        [8.6343],\n",
              "        [6.5268],\n",
              "        [3.1783],\n",
              "        [4.2231],\n",
              "        [7.0570],\n",
              "        [4.0583],\n",
              "        [1.7974],\n",
              "        [1.5284],\n",
              "        [5.4721],\n",
              "        [6.1895],\n",
              "        [6.4263],\n",
              "        [2.5337],\n",
              "        [3.4818],\n",
              "        [5.1249],\n",
              "        [4.0838],\n",
              "        [7.5503],\n",
              "        [4.5774],\n",
              "        [3.2494],\n",
              "        [4.3836],\n",
              "        [5.6150],\n",
              "        [3.7437],\n",
              "        [7.6647],\n",
              "        [4.1741],\n",
              "        [7.4190],\n",
              "        [5.5408],\n",
              "        [8.0407],\n",
              "        [6.0987],\n",
              "        [4.5844],\n",
              "        [6.1918],\n",
              "        [6.6797],\n",
              "        [4.4442],\n",
              "        [4.4009],\n",
              "        [4.6573],\n",
              "        [3.3590],\n",
              "        [3.8129],\n",
              "        [5.7611],\n",
              "        [2.4418],\n",
              "        [2.6931],\n",
              "        [2.7902],\n",
              "        [5.2594],\n",
              "        [6.5633],\n",
              "        [4.5598],\n",
              "        [5.2863],\n",
              "        [4.1026],\n",
              "        [7.4798],\n",
              "        [4.4265],\n",
              "        [6.2309],\n",
              "        [5.1895],\n",
              "        [4.8482],\n",
              "        [5.9834],\n",
              "        [7.3719],\n",
              "        [6.4973],\n",
              "        [4.2668],\n",
              "        [6.6663],\n",
              "        [6.4040],\n",
              "        [4.8495],\n",
              "        [8.3475],\n",
              "        [3.2075],\n",
              "        [2.5216],\n",
              "        [3.2861],\n",
              "        [4.0378],\n",
              "        [8.3851],\n",
              "        [4.0243],\n",
              "        [4.9524],\n",
              "        [3.7453],\n",
              "        [3.9137],\n",
              "        [5.1444],\n",
              "        [3.2070],\n",
              "        [4.4808],\n",
              "        [4.9825],\n",
              "        [1.2434],\n",
              "        [3.7797],\n",
              "        [4.7767],\n",
              "        [5.4018],\n",
              "        [6.0259],\n",
              "        [5.5650],\n",
              "        [6.2159],\n",
              "        [5.3758],\n",
              "        [2.6803],\n",
              "        [5.9401],\n",
              "        [3.1114],\n",
              "        [5.6538],\n",
              "        [4.7429],\n",
              "        [6.4377],\n",
              "        [5.7170],\n",
              "        [5.3844],\n",
              "        [7.7053],\n",
              "        [2.5620],\n",
              "        [6.9530],\n",
              "        [3.8680],\n",
              "        [2.9294],\n",
              "        [6.9665],\n",
              "        [5.5073],\n",
              "        [1.1769],\n",
              "        [6.1373],\n",
              "        [6.1591]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8trxjnSEGXc"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy_50IcNESX5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "27bf7a38-aca1-4f0d-dbd8-41c16e407766"
      },
      "source": [
        "plt.figure()\n",
        "plt.scatter(x[:,0],y)#x[:,0~2] [:1]的貢獻最大(w=5)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cGuNBTxNZuy"
      },
      "source": [
        "w = torch.rand(in_features, out_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IikaNxdn7TwW"
      },
      "source": [
        "# 土法煉鋼"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9iXgM-iNVR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e799764-49db-4290-b4e2-1c9887167cd8"
      },
      "source": [
        "for epoch in range(epoch_n):\n",
        "  y_pred = x.mm(w)\n",
        "  loss = (y_pred-y).pow(2).sum()#為了增加速度才沒有取平均\n",
        "  print(\"Epoch:{},Loss:{:.4f}\".format(epoch,loss))\n",
        "  grad_y_pred = 2*(y_pred-y)\n",
        "  grad_w = x.t().mm(grad_y_pred)#目標是3,1 故將x轉置 3,100x100,1=>3,1\n",
        "  w-=learning_rate*grad_w#透過grad_w與學習速度 更新w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:0,Loss:1797.4558\n",
            "Epoch:1,Loss:1306.3607\n",
            "Epoch:2,Loss:955.5110\n",
            "Epoch:3,Loss:704.6619\n",
            "Epoch:4,Loss:525.1243\n",
            "Epoch:5,Loss:396.4462\n",
            "Epoch:6,Loss:304.0476\n",
            "Epoch:7,Loss:237.5343\n",
            "Epoch:8,Loss:189.4962\n",
            "Epoch:9,Loss:154.6500\n",
            "Epoch:10,Loss:129.2288\n",
            "Epoch:11,Loss:110.5466\n",
            "Epoch:12,Loss:96.6881\n",
            "Epoch:13,Loss:86.2872\n",
            "Epoch:14,Loss:78.3694\n",
            "Epoch:15,Loss:72.2395\n",
            "Epoch:16,Loss:67.4010\n",
            "Epoch:17,Loss:63.4998\n",
            "Epoch:18,Loss:60.2826\n",
            "Epoch:19,Loss:57.5683\n",
            "Epoch:20,Loss:55.2273\n",
            "Epoch:21,Loss:53.1668\n",
            "Epoch:22,Loss:51.3200\n",
            "Epoch:23,Loss:49.6388\n",
            "Epoch:24,Loss:48.0886\n",
            "Epoch:25,Loss:46.6440\n",
            "Epoch:26,Loss:45.2868\n",
            "Epoch:27,Loss:44.0033\n",
            "Epoch:28,Loss:42.7836\n",
            "Epoch:29,Loss:41.6200\n",
            "Epoch:30,Loss:40.5067\n",
            "Epoch:31,Loss:39.4394\n",
            "Epoch:32,Loss:38.4144\n",
            "Epoch:33,Loss:37.4290\n",
            "Epoch:34,Loss:36.4806\n",
            "Epoch:35,Loss:35.5674\n",
            "Epoch:36,Loss:34.6875\n",
            "Epoch:37,Loss:33.8395\n",
            "Epoch:38,Loss:33.0219\n",
            "Epoch:39,Loss:32.2336\n",
            "Epoch:40,Loss:31.4734\n",
            "Epoch:41,Loss:30.7401\n",
            "Epoch:42,Loss:30.0328\n",
            "Epoch:43,Loss:29.3505\n",
            "Epoch:44,Loss:28.6923\n",
            "Epoch:45,Loss:28.0573\n",
            "Epoch:46,Loss:27.4447\n",
            "Epoch:47,Loss:26.8537\n",
            "Epoch:48,Loss:26.2835\n",
            "Epoch:49,Loss:25.7334\n",
            "Epoch:50,Loss:25.2026\n",
            "Epoch:51,Loss:24.6905\n",
            "Epoch:52,Loss:24.1965\n",
            "Epoch:53,Loss:23.7198\n",
            "Epoch:54,Loss:23.2599\n",
            "Epoch:55,Loss:22.8162\n",
            "Epoch:56,Loss:22.3880\n",
            "Epoch:57,Loss:21.9750\n",
            "Epoch:58,Loss:21.5765\n",
            "Epoch:59,Loss:21.1920\n",
            "Epoch:60,Loss:20.8210\n",
            "Epoch:61,Loss:20.4630\n",
            "Epoch:62,Loss:20.1177\n",
            "Epoch:63,Loss:19.7845\n",
            "Epoch:64,Loss:19.4630\n",
            "Epoch:65,Loss:19.1528\n",
            "Epoch:66,Loss:18.8535\n",
            "Epoch:67,Loss:18.5648\n",
            "Epoch:68,Loss:18.2862\n",
            "Epoch:69,Loss:18.0174\n",
            "Epoch:70,Loss:17.7580\n",
            "Epoch:71,Loss:17.5078\n",
            "Epoch:72,Loss:17.2664\n",
            "Epoch:73,Loss:17.0334\n",
            "Epoch:74,Loss:16.8087\n",
            "Epoch:75,Loss:16.5918\n",
            "Epoch:76,Loss:16.3826\n",
            "Epoch:77,Loss:16.1807\n",
            "Epoch:78,Loss:15.9859\n",
            "Epoch:79,Loss:15.7979\n",
            "Epoch:80,Loss:15.6166\n",
            "Epoch:81,Loss:15.4416\n",
            "Epoch:82,Loss:15.2728\n",
            "Epoch:83,Loss:15.1100\n",
            "Epoch:84,Loss:14.9528\n",
            "Epoch:85,Loss:14.8012\n",
            "Epoch:86,Loss:14.6548\n",
            "Epoch:87,Loss:14.5137\n",
            "Epoch:88,Loss:14.3775\n",
            "Epoch:89,Loss:14.2461\n",
            "Epoch:90,Loss:14.1192\n",
            "Epoch:91,Loss:13.9969\n",
            "Epoch:92,Loss:13.8788\n",
            "Epoch:93,Loss:13.7649\n",
            "Epoch:94,Loss:13.6550\n",
            "Epoch:95,Loss:13.5490\n",
            "Epoch:96,Loss:13.4467\n",
            "Epoch:97,Loss:13.3480\n",
            "Epoch:98,Loss:13.2527\n",
            "Epoch:99,Loss:13.1608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKTkREeJ5_lI",
        "outputId": "287b47ec-afd3-4c3e-fc99-2e4e29f10931"
      },
      "source": [
        "w #不等於c 因y有產生bias \n",
        "# tensor([[3.2040], vs3\n",
        "#      [4.8471], vs5 原本貢獻性越大 雜訊影響最低 誤差最小\n",
        "#      [1.7940]]) vs1 原本貢獻性越弱 雜訊影響最大 誤差最大"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.3439],\n",
              "        [5.0180],\n",
              "        [1.6260]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HOyn6yA7dST"
      },
      "source": [
        "#Auto-grad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRaf3G837CdT"
      },
      "source": [
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwtP3xao7rlj"
      },
      "source": [
        "Vx = Variable(x, requires_grad = False)#不需要計算grad 是w需要\n",
        "Vy = Variable(y, requires_grad = False)\n",
        "Vw = Variable(torch.rand(in_features, out_features), requires_grad = True)#Variable預設false"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiR_NCXl8BKt",
        "outputId": "46ea3dd3-eb9f-4796-d0a4-2ec52835655c"
      },
      "source": [
        "for epoch in range(epoch_n):\n",
        "  y_pred = Vx.mm(Vw)\n",
        "  loss = (y_pred-Vy).pow(2).sum()#為了增加速度才沒有取平均\n",
        "  print(\"Epoch:{},Loss:{:.4f}\".format(epoch,loss))\n",
        "  # grad_y_pred = 2*(y_pred-y)\n",
        "  # grad_w = x.t().mm(grad_y_pred)#目標是3,1 故將x轉置 3,100x100,1=>3,1\n",
        "\n",
        "  loss.backward()\n",
        "  Vw.data -= learning_rate * Vw.grad.data #透過grad_w與學習速度 更新w\n",
        "\n",
        "  Vw.grad.data.zero_()#會紀錄並累加各影響x 的obj.grad.data 因此要清空"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:0,Loss:2419.3069\n",
            "Epoch:1,Loss:1741.0367\n",
            "Epoch:2,Loss:1257.0625\n",
            "Epoch:3,Loss:911.6082\n",
            "Epoch:4,Loss:664.9134\n",
            "Epoch:5,Loss:488.6343\n",
            "Epoch:6,Loss:362.5659\n",
            "Epoch:7,Loss:272.3043\n",
            "Epoch:8,Loss:207.5814\n",
            "Epoch:9,Loss:161.0771\n",
            "Epoch:10,Loss:127.5727\n",
            "Epoch:11,Loss:103.3478\n",
            "Epoch:12,Loss:85.7497\n",
            "Epoch:13,Loss:72.8872\n",
            "Epoch:14,Loss:63.4116\n",
            "Epoch:15,Loss:56.3612\n",
            "Epoch:16,Loss:51.0499\n",
            "Epoch:17,Loss:46.9885\n",
            "Epoch:18,Loss:43.8278\n",
            "Epoch:19,Loss:41.3184\n",
            "Epoch:20,Loss:39.2823\n",
            "Epoch:21,Loss:37.5924\n",
            "Epoch:22,Loss:36.1577\n",
            "Epoch:23,Loss:34.9129\n",
            "Epoch:24,Loss:33.8114\n",
            "Epoch:25,Loss:32.8196\n",
            "Epoch:26,Loss:31.9132\n",
            "Epoch:27,Loss:31.0748\n",
            "Epoch:28,Loss:30.2916\n",
            "Epoch:29,Loss:29.5542\n",
            "Epoch:30,Loss:28.8559\n",
            "Epoch:31,Loss:28.1915\n",
            "Epoch:32,Loss:27.5571\n",
            "Epoch:33,Loss:26.9498\n",
            "Epoch:34,Loss:26.3672\n",
            "Epoch:35,Loss:25.8076\n",
            "Epoch:36,Loss:25.2694\n",
            "Epoch:37,Loss:24.7513\n",
            "Epoch:38,Loss:24.2524\n",
            "Epoch:39,Loss:23.7717\n",
            "Epoch:40,Loss:23.3083\n",
            "Epoch:41,Loss:22.8616\n",
            "Epoch:42,Loss:22.4308\n",
            "Epoch:43,Loss:22.0154\n",
            "Epoch:44,Loss:21.6147\n",
            "Epoch:45,Loss:21.2282\n",
            "Epoch:46,Loss:20.8553\n",
            "Epoch:47,Loss:20.4956\n",
            "Epoch:48,Loss:20.1486\n",
            "Epoch:49,Loss:19.8139\n",
            "Epoch:50,Loss:19.4910\n",
            "Epoch:51,Loss:19.1794\n",
            "Epoch:52,Loss:18.8788\n",
            "Epoch:53,Loss:18.5888\n",
            "Epoch:54,Loss:18.3090\n",
            "Epoch:55,Loss:18.0391\n",
            "Epoch:56,Loss:17.7787\n",
            "Epoch:57,Loss:17.5274\n",
            "Epoch:58,Loss:17.2850\n",
            "Epoch:59,Loss:17.0511\n",
            "Epoch:60,Loss:16.8254\n",
            "Epoch:61,Loss:16.6077\n",
            "Epoch:62,Loss:16.3976\n",
            "Epoch:63,Loss:16.1950\n",
            "Epoch:64,Loss:15.9995\n",
            "Epoch:65,Loss:15.8108\n",
            "Epoch:66,Loss:15.6288\n",
            "Epoch:67,Loss:15.4532\n",
            "Epoch:68,Loss:15.2838\n",
            "Epoch:69,Loss:15.1203\n",
            "Epoch:70,Loss:14.9626\n",
            "Epoch:71,Loss:14.8104\n",
            "Epoch:72,Loss:14.6636\n",
            "Epoch:73,Loss:14.5219\n",
            "Epoch:74,Loss:14.3853\n",
            "Epoch:75,Loss:14.2534\n",
            "Epoch:76,Loss:14.1262\n",
            "Epoch:77,Loss:14.0034\n",
            "Epoch:78,Loss:13.8850\n",
            "Epoch:79,Loss:13.7708\n",
            "Epoch:80,Loss:13.6605\n",
            "Epoch:81,Loss:13.5541\n",
            "Epoch:82,Loss:13.4515\n",
            "Epoch:83,Loss:13.3525\n",
            "Epoch:84,Loss:13.2570\n",
            "Epoch:85,Loss:13.1648\n",
            "Epoch:86,Loss:13.0758\n",
            "Epoch:87,Loss:12.9900\n",
            "Epoch:88,Loss:12.9072\n",
            "Epoch:89,Loss:12.8274\n",
            "Epoch:90,Loss:12.7503\n",
            "Epoch:91,Loss:12.6759\n",
            "Epoch:92,Loss:12.6042\n",
            "Epoch:93,Loss:12.5349\n",
            "Epoch:94,Loss:12.4681\n",
            "Epoch:95,Loss:12.4037\n",
            "Epoch:96,Loss:12.3415\n",
            "Epoch:97,Loss:12.2815\n",
            "Epoch:98,Loss:12.2236\n",
            "Epoch:99,Loss:12.1678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ICQRaK-JQn",
        "outputId": "43ae97e1-a5dc-4dfa-8e77-648ee9f35f7d"
      },
      "source": [
        "Vw.data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.3382],\n",
              "        [5.0965],\n",
              "        [1.5405]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO24dl6F-7KE"
      },
      "source": [
        "#nn.Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86aTj4uM-r7C"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yyPSJiM_G_Z"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "  def forward(self, x, w):\n",
        "    y_pred = x.mm(w)\n",
        "    return y_pred\n",
        "  def backward(self):\n",
        "    pass #只做繼承"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iA7oTxrAeOy"
      },
      "source": [
        "model = Model()\n",
        "Vw = Variable(torch.rand(in_features, out_features), requires_grad = True)#Variable預設就是true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hSd0Nq3AxTx",
        "outputId": "b69878a2-e106-4f18-864c-c5b5f580d647"
      },
      "source": [
        "for epoch in range(epoch_n):\n",
        "  y_pred = model(Vx,Vw)\n",
        "  loss = (y_pred-Vy).pow(2).sum()#為了增加速度才沒有取平均\n",
        "  print(\"Epoch:{},Loss:{:.4f}\".format(epoch,loss))\n",
        "  # grad_y_pred = 2*(y_pred-y)\n",
        "  # grad_w = x.t().mm(grad_y_pred)#目標是3,1 故將x轉置 3,100x100,1=>3,1\n",
        "\n",
        "  loss.backward()\n",
        "  Vw.data -= learning_rate * Vw.grad.data #透過grad_w與學習速度 更新w\n",
        "\n",
        "  Vw.grad.data.zero_()#會紀錄並累加各影響x 的obj.grad.data 因此要清空\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:0,Loss:2112.0964\n",
            "Epoch:1,Loss:1519.5560\n",
            "Epoch:2,Loss:1096.7825\n",
            "Epoch:3,Loss:795.0400\n",
            "Epoch:4,Loss:579.5876\n",
            "Epoch:5,Loss:425.6588\n",
            "Epoch:6,Loss:315.5992\n",
            "Epoch:7,Loss:236.8233\n",
            "Epoch:8,Loss:180.3591\n",
            "Epoch:9,Loss:139.8106\n",
            "Epoch:10,Loss:110.6180\n",
            "Epoch:11,Loss:89.5306\n",
            "Epoch:12,Loss:74.2306\n",
            "Epoch:13,Loss:63.0657\n",
            "Epoch:14,Loss:54.8575\n",
            "Epoch:15,Loss:48.7658\n",
            "Epoch:16,Loss:44.1913\n",
            "Epoch:17,Loss:40.7064\n",
            "Epoch:18,Loss:38.0061\n",
            "Epoch:19,Loss:35.8728\n",
            "Epoch:20,Loss:34.1509\n",
            "Epoch:21,Loss:32.7294\n",
            "Epoch:22,Loss:31.5289\n",
            "Epoch:23,Loss:30.4924\n",
            "Epoch:24,Loss:29.5792\n",
            "Epoch:25,Loss:28.7601\n",
            "Epoch:26,Loss:28.0139\n",
            "Epoch:27,Loss:27.3254\n",
            "Epoch:28,Loss:26.6836\n",
            "Epoch:29,Loss:26.0803\n",
            "Epoch:30,Loss:25.5096\n",
            "Epoch:31,Loss:24.9671\n",
            "Epoch:32,Loss:24.4495\n",
            "Epoch:33,Loss:23.9542\n",
            "Epoch:34,Loss:23.4792\n",
            "Epoch:35,Loss:23.0230\n",
            "Epoch:36,Loss:22.5844\n",
            "Epoch:37,Loss:22.1622\n",
            "Epoch:38,Loss:21.7556\n",
            "Epoch:39,Loss:21.3639\n",
            "Epoch:40,Loss:20.9863\n",
            "Epoch:41,Loss:20.6222\n",
            "Epoch:42,Loss:20.2712\n",
            "Epoch:43,Loss:19.9326\n",
            "Epoch:44,Loss:19.6060\n",
            "Epoch:45,Loss:19.2909\n",
            "Epoch:46,Loss:18.9870\n",
            "Epoch:47,Loss:18.6938\n",
            "Epoch:48,Loss:18.4109\n",
            "Epoch:49,Loss:18.1379\n",
            "Epoch:50,Loss:17.8746\n",
            "Epoch:51,Loss:17.6205\n",
            "Epoch:52,Loss:17.3753\n",
            "Epoch:53,Loss:17.1388\n",
            "Epoch:54,Loss:16.9105\n",
            "Epoch:55,Loss:16.6903\n",
            "Epoch:56,Loss:16.4778\n",
            "Epoch:57,Loss:16.2728\n",
            "Epoch:58,Loss:16.0750\n",
            "Epoch:59,Loss:15.8841\n",
            "Epoch:60,Loss:15.7000\n",
            "Epoch:61,Loss:15.5223\n",
            "Epoch:62,Loss:15.3508\n",
            "Epoch:63,Loss:15.1854\n",
            "Epoch:64,Loss:15.0257\n",
            "Epoch:65,Loss:14.8717\n",
            "Epoch:66,Loss:14.7231\n",
            "Epoch:67,Loss:14.5797\n",
            "Epoch:68,Loss:14.4413\n",
            "Epoch:69,Loss:14.3078\n",
            "Epoch:70,Loss:14.1789\n",
            "Epoch:71,Loss:14.0546\n",
            "Epoch:72,Loss:13.9347\n",
            "Epoch:73,Loss:13.8189\n",
            "Epoch:74,Loss:13.7073\n",
            "Epoch:75,Loss:13.5995\n",
            "Epoch:76,Loss:13.4955\n",
            "Epoch:77,Loss:13.3952\n",
            "Epoch:78,Loss:13.2984\n",
            "Epoch:79,Loss:13.2050\n",
            "Epoch:80,Loss:13.1148\n",
            "Epoch:81,Loss:13.0279\n",
            "Epoch:82,Loss:12.9439\n",
            "Epoch:83,Loss:12.8630\n",
            "Epoch:84,Loss:12.7848\n",
            "Epoch:85,Loss:12.7094\n",
            "Epoch:86,Loss:12.6367\n",
            "Epoch:87,Loss:12.5665\n",
            "Epoch:88,Loss:12.4987\n",
            "Epoch:89,Loss:12.4334\n",
            "Epoch:90,Loss:12.3703\n",
            "Epoch:91,Loss:12.3094\n",
            "Epoch:92,Loss:12.2507\n",
            "Epoch:93,Loss:12.1940\n",
            "Epoch:94,Loss:12.1394\n",
            "Epoch:95,Loss:12.0866\n",
            "Epoch:96,Loss:12.0357\n",
            "Epoch:97,Loss:11.9866\n",
            "Epoch:98,Loss:11.9392\n",
            "Epoch:99,Loss:11.8934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rO4Gn8vBZmO",
        "outputId": "99d059c6-145a-4d32-ce1e-aaa7fc765324"
      },
      "source": [
        "Vw.data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.3225],\n",
              "        [5.1287],\n",
              "        [1.5204]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCUm0qWkKa3e"
      },
      "source": [
        "#loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x4mG66TCAx1"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = nn.Linear(in_features, out_features, False)\n",
        "  def forward(self, x):\n",
        "    y_pred = self.linear(x)\n",
        "    return y_pred\n",
        "  def backward(self):\n",
        "    pass #只做繼承"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xxY-sdVChhy",
        "outputId": "c1b010b7-186a-43bc-b97a-e4b4ca039932"
      },
      "source": [
        "model = Model()\n",
        "loss_fn = nn.MSELoss()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (linear): Linear(in_features=3, out_features=1, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcgNJBhJDSZ-",
        "outputId": "7cafb621-b466-418d-ebca-434b7451fdcb"
      },
      "source": [
        "for epoch in range(epoch_n * 100):#\"m\"se收斂速度變成原本百分之一 所以*100\n",
        "  y_pred = model(x) #使用nn.Linear 不需要變成Vx\n",
        "  # loss = (y_pred-Vy).pow(2).sum()#為了增加速度才沒有取平均\n",
        "  loss = loss_fn(y_pred, y)\n",
        "  if (epoch+1)%100 == 0:\n",
        "    print(\"Epoch:{},Loss:{:.4f}\".format(epoch,loss))\n",
        "  model.zero_grad()#會紀錄並累加各影響x 的obj.grad.data 因此要清空\n",
        "  # nn.Linear可能會給起始值 backward前做\n",
        "  loss.backward()\n",
        "  for param in model.parameters():#每層取出參數list\n",
        "    param.data -= learning_rate * param.grad.data\n",
        "  # Vw.data -= learning_rate * Vw.grad.data #透過grad_w與學習速度 更新w\n",
        "  # Vw.grad.data.zero_grad()#會紀錄並累加各影響x 的obj.grad.data 因此要清空\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:99,Loss:22.2649\n",
            "Epoch:199,Loss:16.4504\n",
            "Epoch:299,Loss:12.1886\n",
            "Epoch:399,Loss:9.0639\n",
            "Epoch:499,Loss:6.7719\n",
            "Epoch:599,Loss:5.0899\n",
            "Epoch:699,Loss:3.8546\n",
            "Epoch:799,Loss:2.9466\n",
            "Epoch:899,Loss:2.2784\n",
            "Epoch:999,Loss:1.7858\n",
            "Epoch:1099,Loss:1.4220\n",
            "Epoch:1199,Loss:1.1526\n",
            "Epoch:1299,Loss:0.9524\n",
            "Epoch:1399,Loss:0.8029\n",
            "Epoch:1499,Loss:0.6908\n",
            "Epoch:1599,Loss:0.6061\n",
            "Epoch:1699,Loss:0.5416\n",
            "Epoch:1799,Loss:0.4918\n",
            "Epoch:1899,Loss:0.4531\n",
            "Epoch:1999,Loss:0.4224\n",
            "Epoch:2099,Loss:0.3978\n",
            "Epoch:2199,Loss:0.3777\n",
            "Epoch:2299,Loss:0.3609\n",
            "Epoch:2399,Loss:0.3466\n",
            "Epoch:2499,Loss:0.3343\n",
            "Epoch:2599,Loss:0.3234\n",
            "Epoch:2699,Loss:0.3137\n",
            "Epoch:2799,Loss:0.3049\n",
            "Epoch:2899,Loss:0.2968\n",
            "Epoch:2999,Loss:0.2893\n",
            "Epoch:3099,Loss:0.2822\n",
            "Epoch:3199,Loss:0.2756\n",
            "Epoch:3299,Loss:0.2693\n",
            "Epoch:3399,Loss:0.2633\n",
            "Epoch:3499,Loss:0.2576\n",
            "Epoch:3599,Loss:0.2522\n",
            "Epoch:3699,Loss:0.2470\n",
            "Epoch:3799,Loss:0.2420\n",
            "Epoch:3899,Loss:0.2371\n",
            "Epoch:3999,Loss:0.2325\n",
            "Epoch:4099,Loss:0.2280\n",
            "Epoch:4199,Loss:0.2237\n",
            "Epoch:4299,Loss:0.2196\n",
            "Epoch:4399,Loss:0.2156\n",
            "Epoch:4499,Loss:0.2117\n",
            "Epoch:4599,Loss:0.2080\n",
            "Epoch:4699,Loss:0.2044\n",
            "Epoch:4799,Loss:0.2010\n",
            "Epoch:4899,Loss:0.1977\n",
            "Epoch:4999,Loss:0.1944\n",
            "Epoch:5099,Loss:0.1913\n",
            "Epoch:5199,Loss:0.1884\n",
            "Epoch:5299,Loss:0.1855\n",
            "Epoch:5399,Loss:0.1827\n",
            "Epoch:5499,Loss:0.1800\n",
            "Epoch:5599,Loss:0.1774\n",
            "Epoch:5699,Loss:0.1749\n",
            "Epoch:5799,Loss:0.1725\n",
            "Epoch:5899,Loss:0.1702\n",
            "Epoch:5999,Loss:0.1679\n",
            "Epoch:6099,Loss:0.1658\n",
            "Epoch:6199,Loss:0.1637\n",
            "Epoch:6299,Loss:0.1616\n",
            "Epoch:6399,Loss:0.1597\n",
            "Epoch:6499,Loss:0.1578\n",
            "Epoch:6599,Loss:0.1560\n",
            "Epoch:6699,Loss:0.1543\n",
            "Epoch:6799,Loss:0.1526\n",
            "Epoch:6899,Loss:0.1510\n",
            "Epoch:6999,Loss:0.1494\n",
            "Epoch:7099,Loss:0.1479\n",
            "Epoch:7199,Loss:0.1464\n",
            "Epoch:7299,Loss:0.1450\n",
            "Epoch:7399,Loss:0.1436\n",
            "Epoch:7499,Loss:0.1423\n",
            "Epoch:7599,Loss:0.1411\n",
            "Epoch:7699,Loss:0.1398\n",
            "Epoch:7799,Loss:0.1387\n",
            "Epoch:7899,Loss:0.1375\n",
            "Epoch:7999,Loss:0.1364\n",
            "Epoch:8099,Loss:0.1354\n",
            "Epoch:8199,Loss:0.1344\n",
            "Epoch:8299,Loss:0.1334\n",
            "Epoch:8399,Loss:0.1324\n",
            "Epoch:8499,Loss:0.1315\n",
            "Epoch:8599,Loss:0.1306\n",
            "Epoch:8699,Loss:0.1298\n",
            "Epoch:8799,Loss:0.1289\n",
            "Epoch:8899,Loss:0.1281\n",
            "Epoch:8999,Loss:0.1274\n",
            "Epoch:9099,Loss:0.1266\n",
            "Epoch:9199,Loss:0.1259\n",
            "Epoch:9299,Loss:0.1252\n",
            "Epoch:9399,Loss:0.1246\n",
            "Epoch:9499,Loss:0.1239\n",
            "Epoch:9599,Loss:0.1233\n",
            "Epoch:9699,Loss:0.1227\n",
            "Epoch:9799,Loss:0.1221\n",
            "Epoch:9899,Loss:0.1216\n",
            "Epoch:9999,Loss:0.1210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JL--2aVG8oC",
        "outputId": "ef73c3b2-e67a-49c9-f619-ff358b32f663"
      },
      "source": [
        "for param in model.parameters():#每層取出參數list\n",
        "  # param.data -= learning_rate * param.grad.data\n",
        "  print(param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[3.3957, 5.0764, 1.4993]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ7AIqmZLIqv"
      },
      "source": [
        "#oprimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vumx7HoLG-7x",
        "outputId": "02ed165d-92bf-4cc7-b416-f0213a12a1a3"
      },
      "source": [
        "model = Model()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (linear): Linear(in_features=3, out_features=1, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EApgKzC0H1lF",
        "outputId": "0995ee49-6038-408b-fc31-a61e27aa5774"
      },
      "source": [
        "for param in model.parameters():#每層取出參數list\n",
        "  # param.data -= learning_rate * param.grad.data\n",
        "  print(param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.1658, 0.0899, 0.5124]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYVbCoRZH4K7"
      },
      "source": [
        "import torch.optim as opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxBbg2M4IFHq"
      },
      "source": [
        "optimizer = opt.SGD(model.parameters(), lr=learning_rate)#SGD速度比GD快 nn沒提供GD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuyUT5_GIkzk"
      },
      "source": [
        "for epoch in range(epoch_n * 100):#\"m\"se收斂速度變成原本百分之一 所以*100\n",
        "  y_pred = model(x) #使用nn.Linear 不需要變成Vx\n",
        "\n",
        "  loss = loss_fn(y_pred, y)\n",
        "  if (epoch+1)%100 == 0:\n",
        "    print(\"Epoch:{},Loss:{:.4f}\".format(epoch,loss))\n",
        "  model.zero_grad()#會紀錄並累加各影響x 的obj.grad.data 因此要清空\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  # for param in model.parameters():#每層取出參數list\n",
        "  #   param.data -= learning_rate * param.grad.data\n",
        "  # Vw.data -= learning_rate * Vw.grad.data #透過grad_w與學習速度 更新w\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LeypFvVJMHw",
        "outputId": "36a4fd11-896c-43e2-f57b-31e07c6c769c"
      },
      "source": [
        "for param in model.parameters():#每層取出參數list\n",
        "  # param.data -= learning_rate * param.grad.data\n",
        "  print(param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[3.2921, 5.0636, 1.6309]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJXA2vZqQgrg"
      },
      "source": [
        "#digitize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFc7TKroOk0W"
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQN94GTWPw40",
        "outputId": "938ac47f-5200-4d2a-ac7a-189f89982258"
      },
      "source": [
        "npy = y.numpy()\n",
        "bins = np.array([npy.mean()])#依照平均值做分割點 臨界值\n",
        "# print(npy)\n",
        "print(bins)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5.00033]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59wWIWJSOqbY"
      },
      "source": [
        "# npy = y.numpy()\n",
        "# bins = np.array([npy.mean()])\n",
        "inds = np.digitize(npy, bins)#離散化 array, 臨界點\n",
        "cy = torch.from_numpy(inds)\n",
        "cy = cy.float()\n",
        "cy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKdoWLvQQmQW"
      },
      "source": [
        "#autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBkGrTBfQtBa"
      },
      "source": [
        "Vx = Variable(x, requires_grad=False)\n",
        "Vy = Variable(cy, requires_grad=False)\n",
        "Vw = Variable(torch.rand(in_features, out_features), requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "317F1WONTaVz"
      },
      "source": [
        "for epoch in range(epoch_n*1000):\n",
        "  y_pred = torch.sigmoid(Vx.mm(Vw))\n",
        "  loss = (-1.0*(Vy*torch.log10(y_pred)\n",
        "      +(1.0-Vy)*torch.log10(1.0-y_pred))).mean()\n",
        "  if (epoch+1)%1000 == 0: \n",
        "    print(\"Epoch:{},Loss:{:.4f}\".format(epoch,loss))\n",
        "\n",
        "  loss.backward()\n",
        "  Vw.data -= learning_rate * Vw.grad.data #透過grad_w與學習速度 更新w\n",
        "\n",
        "  Vw.grad.data.zero_()#會紀錄並累加各影響x 的obj.grad.data 因此要清空"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPT-GoCMV54X",
        "outputId": "92447008-2354-4613-fd03-24fceb271488"
      },
      "source": [
        "Vw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3068],\n",
              "        [ 1.4135],\n",
              "        [-1.0606]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-mTNbVsQsOL"
      },
      "source": [
        "#torch.nn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7qEquCYWXKj"
      },
      "source": [
        "class LoigisticRegression(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LoigisticRegression, self).__init__()\n",
        "  def forward(self, x, w):\n",
        "    y_pred = torch.sigmoid(x.mm(w))\n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4r1QtUdW_Ot"
      },
      "source": [
        "model = LoigisticRegression()\n",
        "Vw = torch.rand(in_features, out_features , requires_grad=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHM0Dm80XM_4"
      },
      "source": [
        "for epoch in range(epoch_n*1000):\n",
        "  y_pred = model(Vx,Vw)\n",
        "  loss = (-1.0*(Vy*torch.log10(y_pred)\n",
        "      +(1.0-Vy)*torch.log10(1.0-y_pred))).mean()\n",
        "  if (epoch+1)%1000 == 0: \n",
        "    print(\"Epoch:{},Loss:{:.4f}\".format(epoch,loss))\n",
        "\n",
        "  loss.backward()\n",
        "  Vw.data -= learning_rate * Vw.grad.data #透過grad_w與學習速度 更新w\n",
        "\n",
        "  Vw.grad.data.zero_()#會紀錄並累加各影響x 的obj.grad.data 因此要清空"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnHK8t-lQzRC"
      },
      "source": [
        "class LoigisticRegression(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LoigisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(in_features,out_features,False)\n",
        "  def forward(self, x):\n",
        "    # y_pred = torch.sigmoid(x.mm(w))\n",
        "    y_pred = torch.sigmoid(self.linear(x))\n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRegAtCURZxL",
        "outputId": "2f0560d6-48df-4c7e-d68b-4279a3860035"
      },
      "source": [
        "model = LoigisticRegression()\n",
        "loss_fn = nn.BCELoss()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LoigisticRegression(\n",
            "  (linear): Linear(in_features=3, out_features=1, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENJaLB7uRtik",
        "outputId": "c1225012-5c79-4d7b-a8dc-560384f0b145"
      },
      "source": [
        "for epoch in range(epoch_n*1000):\n",
        "  y_pred = model(Vx)\n",
        "  loss = loss_fn(y_pred,Vy)\n",
        "  # loss = (-1.0*(Vy*torch.log10(y_pred)\n",
        "  #     +(1.0-Vy)*torch.log10(1.0-y_pred))).mean()\n",
        "  if (epoch+1)%1000 == 0: \n",
        "    print(\"Epoch:{},Loss:{:.4f}\".format(epoch,loss))\n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  for param in model.parameters():\n",
        "    param.data -= learning_rate * param.grad.data #透過grad_w與學習速度 更新w\n",
        "\n",
        "  # Vw.grad.data.zero_()#會紀錄並累加各影響x 的obj.grad.data 因此要清空"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:999,Loss:0.7256\n",
            "Epoch:1999,Loss:0.7099\n",
            "Epoch:2999,Loss:0.6982\n",
            "Epoch:3999,Loss:0.6894\n",
            "Epoch:4999,Loss:0.6824\n",
            "Epoch:5999,Loss:0.6768\n",
            "Epoch:6999,Loss:0.6722\n",
            "Epoch:7999,Loss:0.6682\n",
            "Epoch:8999,Loss:0.6648\n",
            "Epoch:9999,Loss:0.6617\n",
            "Epoch:10999,Loss:0.6588\n",
            "Epoch:11999,Loss:0.6562\n",
            "Epoch:12999,Loss:0.6538\n",
            "Epoch:13999,Loss:0.6515\n",
            "Epoch:14999,Loss:0.6493\n",
            "Epoch:15999,Loss:0.6473\n",
            "Epoch:16999,Loss:0.6453\n",
            "Epoch:17999,Loss:0.6435\n",
            "Epoch:18999,Loss:0.6417\n",
            "Epoch:19999,Loss:0.6400\n",
            "Epoch:20999,Loss:0.6384\n",
            "Epoch:21999,Loss:0.6368\n",
            "Epoch:22999,Loss:0.6353\n",
            "Epoch:23999,Loss:0.6339\n",
            "Epoch:24999,Loss:0.6325\n",
            "Epoch:25999,Loss:0.6311\n",
            "Epoch:26999,Loss:0.6299\n",
            "Epoch:27999,Loss:0.6287\n",
            "Epoch:28999,Loss:0.6275\n",
            "Epoch:29999,Loss:0.6263\n",
            "Epoch:30999,Loss:0.6253\n",
            "Epoch:31999,Loss:0.6242\n",
            "Epoch:32999,Loss:0.6232\n",
            "Epoch:33999,Loss:0.6223\n",
            "Epoch:34999,Loss:0.6213\n",
            "Epoch:35999,Loss:0.6204\n",
            "Epoch:36999,Loss:0.6196\n",
            "Epoch:37999,Loss:0.6188\n",
            "Epoch:38999,Loss:0.6180\n",
            "Epoch:39999,Loss:0.6172\n",
            "Epoch:40999,Loss:0.6165\n",
            "Epoch:41999,Loss:0.6158\n",
            "Epoch:42999,Loss:0.6151\n",
            "Epoch:43999,Loss:0.6144\n",
            "Epoch:44999,Loss:0.6138\n",
            "Epoch:45999,Loss:0.6132\n",
            "Epoch:46999,Loss:0.6126\n",
            "Epoch:47999,Loss:0.6120\n",
            "Epoch:48999,Loss:0.6115\n",
            "Epoch:49999,Loss:0.6110\n",
            "Epoch:50999,Loss:0.6105\n",
            "Epoch:51999,Loss:0.6100\n",
            "Epoch:52999,Loss:0.6095\n",
            "Epoch:53999,Loss:0.6091\n",
            "Epoch:54999,Loss:0.6086\n",
            "Epoch:55999,Loss:0.6082\n",
            "Epoch:56999,Loss:0.6078\n",
            "Epoch:57999,Loss:0.6074\n",
            "Epoch:58999,Loss:0.6071\n",
            "Epoch:59999,Loss:0.6067\n",
            "Epoch:60999,Loss:0.6063\n",
            "Epoch:61999,Loss:0.6060\n",
            "Epoch:62999,Loss:0.6057\n",
            "Epoch:63999,Loss:0.6054\n",
            "Epoch:64999,Loss:0.6051\n",
            "Epoch:65999,Loss:0.6048\n",
            "Epoch:66999,Loss:0.6045\n",
            "Epoch:67999,Loss:0.6042\n",
            "Epoch:68999,Loss:0.6040\n",
            "Epoch:69999,Loss:0.6037\n",
            "Epoch:70999,Loss:0.6035\n",
            "Epoch:71999,Loss:0.6032\n",
            "Epoch:72999,Loss:0.6030\n",
            "Epoch:73999,Loss:0.6028\n",
            "Epoch:74999,Loss:0.6026\n",
            "Epoch:75999,Loss:0.6024\n",
            "Epoch:76999,Loss:0.6022\n",
            "Epoch:77999,Loss:0.6020\n",
            "Epoch:78999,Loss:0.6018\n",
            "Epoch:79999,Loss:0.6016\n",
            "Epoch:80999,Loss:0.6014\n",
            "Epoch:81999,Loss:0.6013\n",
            "Epoch:82999,Loss:0.6011\n",
            "Epoch:83999,Loss:0.6009\n",
            "Epoch:84999,Loss:0.6008\n",
            "Epoch:85999,Loss:0.6007\n",
            "Epoch:86999,Loss:0.6005\n",
            "Epoch:87999,Loss:0.6004\n",
            "Epoch:88999,Loss:0.6002\n",
            "Epoch:89999,Loss:0.6001\n",
            "Epoch:90999,Loss:0.6000\n",
            "Epoch:91999,Loss:0.5999\n",
            "Epoch:92999,Loss:0.5998\n",
            "Epoch:93999,Loss:0.5996\n",
            "Epoch:94999,Loss:0.5995\n",
            "Epoch:95999,Loss:0.5994\n",
            "Epoch:96999,Loss:0.5993\n",
            "Epoch:97999,Loss:0.5992\n",
            "Epoch:98999,Loss:0.5991\n",
            "Epoch:99999,Loss:0.5991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-ThE8ZkS69j",
        "outputId": "ca667a47-26a5-49d6-802a-99e39aa9fe84"
      },
      "source": [
        "for param in model.parameters():#每層取出參數list\n",
        "  # param.data -= learning_rate * param.grad.data\n",
        "  print(param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3261,  1.8244, -1.5217]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvts89IlTBly"
      },
      "source": [
        "model = LoigisticRegression()\n",
        "optimizer = opt.SGD(model.parameters(),lr=learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW0LMHEFTbTv",
        "outputId": "674b07f2-36d7-4405-db96-d0cfd61540c6"
      },
      "source": [
        "for epoch in range(epoch_n*1000):\n",
        "  y_pred = model(Vx)\n",
        "  loss = loss_fn(y_pred,Vy)\n",
        "\n",
        "  if (epoch+1)%1000 == 0: \n",
        "    print(\"Epoch:{},Loss:{:.4f}\".format(epoch,loss))\n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  # for param in model.parameters():\n",
        "  #   param.data -= learning_rate * param.grad.data #透過grad_w與學習速度 更新w\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:999,Loss:0.6986\n",
            "Epoch:1999,Loss:0.6899\n",
            "Epoch:2999,Loss:0.6831\n",
            "Epoch:3999,Loss:0.6775\n",
            "Epoch:4999,Loss:0.6729\n",
            "Epoch:5999,Loss:0.6689\n",
            "Epoch:6999,Loss:0.6654\n",
            "Epoch:7999,Loss:0.6623\n",
            "Epoch:8999,Loss:0.6594\n",
            "Epoch:9999,Loss:0.6568\n",
            "Epoch:10999,Loss:0.6543\n",
            "Epoch:11999,Loss:0.6520\n",
            "Epoch:12999,Loss:0.6498\n",
            "Epoch:13999,Loss:0.6477\n",
            "Epoch:14999,Loss:0.6457\n",
            "Epoch:15999,Loss:0.6438\n",
            "Epoch:16999,Loss:0.6420\n",
            "Epoch:17999,Loss:0.6403\n",
            "Epoch:18999,Loss:0.6386\n",
            "Epoch:19999,Loss:0.6370\n",
            "Epoch:20999,Loss:0.6355\n",
            "Epoch:21999,Loss:0.6341\n",
            "Epoch:22999,Loss:0.6327\n",
            "Epoch:23999,Loss:0.6313\n",
            "Epoch:24999,Loss:0.6300\n",
            "Epoch:25999,Loss:0.6288\n",
            "Epoch:26999,Loss:0.6276\n",
            "Epoch:27999,Loss:0.6265\n",
            "Epoch:28999,Loss:0.6254\n",
            "Epoch:29999,Loss:0.6243\n",
            "Epoch:30999,Loss:0.6233\n",
            "Epoch:31999,Loss:0.6223\n",
            "Epoch:32999,Loss:0.6214\n",
            "Epoch:33999,Loss:0.6205\n",
            "Epoch:34999,Loss:0.6196\n",
            "Epoch:35999,Loss:0.6188\n",
            "Epoch:36999,Loss:0.6180\n",
            "Epoch:37999,Loss:0.6172\n",
            "Epoch:38999,Loss:0.6165\n",
            "Epoch:39999,Loss:0.6158\n",
            "Epoch:40999,Loss:0.6151\n",
            "Epoch:41999,Loss:0.6144\n",
            "Epoch:42999,Loss:0.6138\n",
            "Epoch:43999,Loss:0.6132\n",
            "Epoch:44999,Loss:0.6126\n",
            "Epoch:45999,Loss:0.6120\n",
            "Epoch:46999,Loss:0.6115\n",
            "Epoch:47999,Loss:0.6109\n",
            "Epoch:48999,Loss:0.6104\n",
            "Epoch:49999,Loss:0.6100\n",
            "Epoch:50999,Loss:0.6095\n",
            "Epoch:51999,Loss:0.6090\n",
            "Epoch:52999,Loss:0.6086\n",
            "Epoch:53999,Loss:0.6082\n",
            "Epoch:54999,Loss:0.6078\n",
            "Epoch:55999,Loss:0.6074\n",
            "Epoch:56999,Loss:0.6070\n",
            "Epoch:57999,Loss:0.6066\n",
            "Epoch:58999,Loss:0.6063\n",
            "Epoch:59999,Loss:0.6060\n",
            "Epoch:60999,Loss:0.6056\n",
            "Epoch:61999,Loss:0.6053\n",
            "Epoch:62999,Loss:0.6050\n",
            "Epoch:63999,Loss:0.6047\n",
            "Epoch:64999,Loss:0.6044\n",
            "Epoch:65999,Loss:0.6042\n",
            "Epoch:66999,Loss:0.6039\n",
            "Epoch:67999,Loss:0.6037\n",
            "Epoch:68999,Loss:0.6034\n",
            "Epoch:69999,Loss:0.6032\n",
            "Epoch:70999,Loss:0.6030\n",
            "Epoch:71999,Loss:0.6027\n",
            "Epoch:72999,Loss:0.6025\n",
            "Epoch:73999,Loss:0.6023\n",
            "Epoch:74999,Loss:0.6021\n",
            "Epoch:75999,Loss:0.6019\n",
            "Epoch:76999,Loss:0.6017\n",
            "Epoch:77999,Loss:0.6016\n",
            "Epoch:78999,Loss:0.6014\n",
            "Epoch:79999,Loss:0.6012\n",
            "Epoch:80999,Loss:0.6011\n",
            "Epoch:81999,Loss:0.6009\n",
            "Epoch:82999,Loss:0.6008\n",
            "Epoch:83999,Loss:0.6006\n",
            "Epoch:84999,Loss:0.6005\n",
            "Epoch:85999,Loss:0.6003\n",
            "Epoch:86999,Loss:0.6002\n",
            "Epoch:87999,Loss:0.6001\n",
            "Epoch:88999,Loss:0.6000\n",
            "Epoch:89999,Loss:0.5998\n",
            "Epoch:90999,Loss:0.5997\n",
            "Epoch:91999,Loss:0.5996\n",
            "Epoch:92999,Loss:0.5995\n",
            "Epoch:93999,Loss:0.5994\n",
            "Epoch:94999,Loss:0.5993\n",
            "Epoch:95999,Loss:0.5992\n",
            "Epoch:96999,Loss:0.5991\n",
            "Epoch:97999,Loss:0.5990\n",
            "Epoch:98999,Loss:0.5989\n",
            "Epoch:99999,Loss:0.5988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klqD21AUUIE7",
        "outputId": "6c39d465-c518-46f7-d968-d9196ca4e941"
      },
      "source": [
        "for param in model.parameters():#每層取出參數list\n",
        "  # param.data -= learning_rate * param.grad.data\n",
        "  print(param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3690,  1.8120, -1.5538]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}